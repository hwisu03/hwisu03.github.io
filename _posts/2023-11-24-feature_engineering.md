---
use_math : True
---
# 1.í”¼ì³ì—”ì§€ë‹ˆì–´ë§
## 1-1. í”¼ì³(feature)
ë°ì´í„°ëª¨ë¸(íŠ¹íˆ, ì¸ê³µì§€ëŠ¥)ì—ì„œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ì…ë ¥ë³€ìˆ˜ë¥¼ ì˜ë¯¸í•œë‹¤. <br>
í†µê³„í•™ì—ì„œëŠ” ë…ë¦½ë³€ìˆ˜ë¼ê³  í•œë‹¤. <br>
<br>
ìƒê´€ê´€ê³„: ë…ë¦½ë³€ìˆ˜<->ì¢…ì†ë³€ìˆ˜<br>
ì¸ê³¼ê´€ê³„: ë…ë¦½ë³€ìˆ˜->ì¢…ì†ë³€ìˆ˜<br>
<br>
ì¸ê³¼ê´€ê³„ê°€ ë˜ê¸° ìœ„í•œ ì¡°ê±´<br>
1. x, yê°€ ìƒê´€ê´€ê³„<br>
2. ì‹œê°„ì  ì„ í›„ê´€ê³„ï¼ˆxê°€ ë¨¼ì €)<br>
3. ë¹„í—ˆìœ„ì  ê´€ê³„(non-spurious)<br>
<br>
ğŸ’ í”¼ì³ì˜ ìœ í˜•<br>
-ì†ì„±ì— ë”°ë¼ ë²”ì£¼í˜•(ë²”ì£¼ë‚˜ ìˆœìœ„ê°€ ìˆëŠ” ë³€ìˆ˜), ìˆ˜ì¹˜í˜•(ìˆ˜ì¹˜ë¡œ í‘œí˜„ë˜ëŠ” ë³€ìˆ˜)ë¡œ ë‚˜ë‰œë‹¤.
<br>
<br>
ì¸ê³¼ê´€ê³„ì— ë”°ë¼ ë…ë¦½ë³€ìˆ˜(ë‹¤ë¥¸ ë³€ìˆ˜ì— ì˜í–¥ì„ ë°›ì§€ ì•Šê³  ì¢…ì†ë³€ìˆ˜ì— ì˜í–¥ì„ ì£¼ëŠ” ë³€ìˆ˜), ì¢…ì†ë³€ìˆ˜(ë…ë¦½ ë³€ìˆ˜ë¡œë¶€í„° ì˜í–¥ì„ ë°›ëŠ” ë³€ìˆ˜)ë¡œ ë‚˜ë‰œë‹¤.
<br>
<br>
ë¨¸ì‹ ëŸ¬ë‹ì—ì„œì˜ ì…ë ¥: ë³€ìˆ˜(Feature), ì†ì„±(Attribute), ì˜ˆì¸¡ë³€ìˆ˜(Predictor), ì°¨ì›(Dimension), ê´€ì¸¡ì¹˜(Observation), ë…ë¦½ë³€ìˆ˜(Independent Variable)<br>
ë¨¸ì‹ ëŸ¬ë‹ì—ì„œì˜ ì¶œë ¥: ë¼ë²¨(Label), í´ë˜ìŠ¤(Class), ëª©í‘¯ê°’(Target), ë°˜ì‘(Response), ì¢…ì†ë³€ìˆ˜(Dependent Variable)<br>
<br>

## 1-2. í”¼ì³ ì—”ì§€ë‹ˆì–´ë§(Feature Engineering)
í”¼ì³ ì—”ì§€ë‹ˆì–´ë§ì€ ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•˜ì—¬ ë°ì´í„°ì— ëŒ€í•œ ë„ë©”ì¸ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ë³€ìˆ˜ë¥¼ ì¡°í•©í•˜ê±°ë‚˜ ìƒˆë¡œìš´ ë³€ìˆ˜ë¥¼ ë§Œë“œëŠ” ê³¼ì •ì„ ë§í•œë‹¤.<br>
í”¼ì³ ì¶”ì¶œ, í”¼ì³ ì„ íƒ ëª¨ë‘ í”¼ì³ì˜ ê°œìˆ˜ë¥¼ ì¤„ì´ëŠ” ë°©ë²•ì´ë‹¤.<br>
<br>
<br>
ğŸ’ í”¼ì³ì¶”ì¶œ(feature extraction): ìƒˆë¡œìš´ í”¼ì³ë¥¼ ë§Œë“œëŠ” ë°©ë²•<br>
-í”¼ì³ë“¤ ì‚¬ì´ì— ë‚´ì¬í•œ íŠ¹ì„±ì´ë‚˜ ê´€ê³„ë¥¼ ë¶„ì„í•˜ì—¬ ì´ë“¤ì„ ì˜ í‘œí˜„í•  ìˆ˜ ìˆëŠ” ìƒˆë¡œìš´ ì„ í˜• í˜¹ì€ ë¹„ì„ í˜• ê²°í•© ë³€ìˆ˜ë¥¼ ë§Œë“¤ì–´ ë°ì´í„°ë¥¼ ì¤„ì´ëŠ” ë°©ë²•<br>
-ê³ ì°¨ì›ì˜ ì›ë³¸ í”¼ì³ ê³µê°„ì„ ì €ì°¨ì›ì˜ ìƒˆë¡œìš´ í”¼ì³ ê³µê°„ìœ¼ë¡œ íˆ¬ì˜<br>
-PCA(ì£¼ì„±ë¶„ ë¶„ì„), LDA(ì„ í˜• íŒë³„ ë¶„ì„) ë“±<br>

ğŸ’ í”¼ì³ ì„ íƒ(feature selection): í”¼ì³ì˜ ìˆ˜ë¥¼ ì¤„ì´ëŠ” ë°©ë²•<br>
-í”¼ì³ ì¤‘ íƒ€ê²Ÿì— ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ í”¼ì³ë§Œì„ ì„ ì •í•˜ì—¬ í”¼ì³ì˜ ìˆ˜ë¥¼ ì¤„ì´ëŠ” ë°©ë²•<br>
-ê´€ë ¨ì—†ê±°ë‚˜ ì¤‘ë³µë˜ëŠ” í”¼ì³ë“¤ì„ í•„í„°ë§í•˜ê³  ê°„ê²°í•œ subsetì„ ìƒì„±<br>
-ëª¨ë¸ ë‹¨ìˆœí™”, í›ˆë ¨ ì‹œê°„ ì¶•ì†Œ, ì°¨ì›ì˜ ì €ì£¼ ë°©ì§€, ê³¼ì í•©(Over-fitting)ì„ ì¤„ì—¬ ì¼ë°˜í™”í•´ì£¼ëŠ” ì¥ì ì´ ìˆìŒ<br>
-Filter, Wrapper, Embedded ë©”ì„œë“œ<br>
![png](https://drive.google.com/uc?id=1gjZXN4-EPa_tRuIkgRismbiFpfs9ODiN)
![png](https://drive.google.com/uc?id=1m9NRuVkNnJKL0gjt7lHR8ABRJ49uD0u6)<br>
ì‚¬ì§„ì„ ë³´ë©´ Feature Engineeringì˜ ë°©ë²•ì´ ì—„ì²­ë‚˜ê²Œ ë§ì€ë°, ì´ëŠ” dominantí•œ ë°©ë²•ì´ ì—†ë‹¤ëŠ” ê²ƒì„ ëœ»í•œë‹¤.<br>
# 2. í”¼ì³ ì¶”ì¶œ
## 2-1.í”¼ì³ ì¶”ì¶œ(Feature Extraction)<br>
ë³€ìˆ˜ë“¤ ì‚¬ì´ì— ë‚´ì¬í•œ íŠ¹ì„±ì´ë‚˜ ê´€ê³„ë¥¼ ë¶„ì„í•˜ì—¬ ì´ë“¤ì„ ì˜ í‘œí˜„í•  ìˆ˜ ìˆëŠ” ìƒˆë¡œìš´ ì„ í˜• í˜¹ì€ ë¹„ì„ í˜• ê²°í•© ë³€ìˆ˜ë¥¼ ë§Œë“¤ì–´ ë°ì´í„°ë¥¼ ì¤„ì´ëŠ” ë°©ë²•<br>
<br>
ì£¼ì„±ë¶„ ë¶„ì„(Principal Component Analysis, PCA)<br>
-ë³€ìˆ˜ë“¤ì˜ ê³µë¶„ì‚° í–‰ë ¬ì´ë‚˜ ìƒê´€í–‰ë ¬ì„ ì´ìš©<br>
-ì›ë˜ ë°ì´í„° íŠ¹ì§•ì„ ì˜ ì„¤ëª…í•´ì£¼ëŠ” ì„±ë¶„ì„ ì¶”ì¶œí•˜ê¸° ì´í•˜ì—¬ ê³ ì°¨ì› ê³µê°„ì˜ í‘œë³¸ë“¤ì„ ì„ í˜• ì—°ê´€ì„±ì´ ì—†ëŠ” ì €ì°¨ì› ê³µê°„ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê¸°ë²•<br>
-í–‰ì˜ ìˆ˜ì™€ ì—´ì˜ ìˆ˜ê°€ ê°™ì€ ì •ë°©í–‰ë ¬ì—ì„œë§Œ ì‚¬ìš©<br>
<br>
ì„ í˜• íŒë³„ ë¶„ì„(Linear Discriminant Analysis, LDA)<br>
-ë°ì´í„°ì˜ Targetê°’ í´ë˜ìŠ¤ë¼ë¦¬ ìµœëŒ€í•œ ë¶„ë¦¬í•  ìˆ˜ ìˆëŠ” ì¶•ì„ ì°¾ìŒ<br>
-íŠ¹ì • ê³µê°„ìƒì—ì„œ í´ë˜ìŠ¤ ë¶„ë¦¬ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ì¶•ì„ ì°¾ê¸° ìœ„í•´ í´ë˜ìŠ¤ ê°„ ë¶„ì‚°(between-class scatter)ê³¼ í´ë˜ìŠ¤ ë‚´ë¶€ ë¶„ì‚°(within-class scatter)ì˜ ë¹„ìœ¨ì„ ìµœëŒ€í™”í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì°¨ì›ì„ ì¶•ì†Œ<br>
<br>
íŠ¹ì´ê°’ ë¶„í•´(Singular Value Decomposition)<br>
-M X N ì°¨ì›ì˜ í–‰ë ¬ë°ì´í„°ì—ì„œ íŠ¹ì´ê°’ì„ ì¶”ì¶œí•˜ê³  ì´ë¥¼ í†µí•´ ì£¼ì–´ì§„ ë°ì´í„° ì„¸íŠ¸ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì¶•ì•½í•  ìˆ˜ ìˆëŠ” ê¸°ë²•<br>
<br>
ìš”ì¸ ë¶„ì„(Factor Analysis)<br>
-ë°ì´í„° ì•ˆì— ê´€ì°°í•  ìˆ˜ ìˆëŠ” ì ì¬ì ì¸ ë³€ìˆ˜(Latent Variable)ê°€ ì¡´ì¬í•œë‹¤ê³  ê°€ì •<br>
-ëª¨í˜•ì„ ì„¸ìš´ ë’¤ ê´€ì°° ê°€ëŠ¥í•œ ë°ì´í„°ë¥¼ ì´ìš©í•˜ì—¬ í•´ë‹¹ ì ì¬ ìš”ì¸ì„ ë„ì¶œí•˜ê³  ë°ì´í„° ì•ˆì˜ êµ¬ì¡°ë¥¼ í•´ì„í•˜ëŠ” ê¸°ë²•<br>
-ì£¼ë¡œ ì‚¬íšŒê³¼í•™ì´ë‚˜ ì„¤ë¬¸ ì¡°ì‚¬ ë“±ì—ì„œ ë§ì´ í™œìš©<br>
<br>
ë…ë¦½ ì„±ë¶„ ë¶„ì„(Independent Component Analysis)<br>
-ì£¼ì„±ë¶„ ë¶„ì„ê³¼ëŠ” ë‹¬ë¦¬ ë‹¤ë³€ëŸ‰ì˜ ì‹ í˜¸ë¥¼ í†µê³„ì ìœ¼ë¡œ ë…ë¦½ì ì¸ í•˜ë¶€ì„±ë¶„ìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬ ì°¨ì›ì„ ì¶•ì†Œí•˜ëŠ” ê¸°ë²•<br>
-ë…ë¦½ ì„±ë¶„ì˜ ë¶„í¬ëŠ” ë¹„ì •ê·œ ë¶„í¬ë¥¼ ë”°ë¥´ê²Œ ë˜ëŠ” ì°¨ì›ì¶•ì†Œ ê¸°ë²•<br>
<br>
ë‹¤ì°¨ì› ì²™ë„ë²•(Multi-Dimensional Scaling)<br>
-ê°œì²´ë“¤ ì‚¬ì´ì˜ ìœ ì‚¬ì„±, ë¹„ìœ ì‚¬ì„±ì„ ì¸¡ì •í•˜ì—¬ 2ì°¨ì› ë˜ëŠ” 3ì°¨ì› ê³µê°„ìƒì— ì ìœ¼ë¡œ í‘œí˜„í•˜ì—¬ ê°œì²´ë“¤ ì‚¬ì´ì˜ ì§‘ë‹¨í™”ë¥¼ ì‹œê°ì ìœ¼ë¡œ í‘œí˜„í•˜ëŠ” ë¶„ì„ ë°©ë²•<br>
<br>
## 2-2 ì£¼ì„±ë¶„ ë¶„ì„
ğŸ’  ì£¼ì„±ë¶„ ë¶„ì„(Principal Component Analysis)<br>
<br>
ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ì°¨ì›(ë³€ìˆ˜) ì¶•ì†Œ ê¸°ë²• ì¤‘ í•˜ë‚˜<br>
ì› ë°ì´í„°ì˜ ë¶„ì‚°(variance)ì„ ìµœëŒ€í•œ ë³´ì¡´í•˜ë©´ì„œ ì„œë¡œ ì§êµí•˜ëŠ” ìƒˆ ê¸°ì €(ì¶•)ë¥¼ ì°¾ì•„, ê³ ì°¨ì› ê³µê°„ì˜ í‘œë³¸ë“¤ì„ ì„ í˜• ì—°ê´€ì„±ì´ ì—†ëŠ” ì €ì°¨ì› ê³µê°„ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê¸°ë²•<br>
PCAëŠ” ê¸°ì¡´ì˜ ë³€ìˆ˜ë¥¼ ì¡°í•©í•˜ì—¬ ì„œë¡œ ì—°ê´€ì„±ì´ ì—†ëŠ” ìƒˆë¡œìš´ ë³€ìˆ˜, ì¦‰ ì£¼ì„±ë¶„(principal component, PC)ë“¤ì„ ë§Œë“¤ì–´ ëƒ„<br>
ì£¼ì„±ë¶„ì˜ ê°œìˆ˜ë¥¼ ì¦ê°€ì‹œí‚´ì— ë”°ë¼ ì› ë°ì´í„°ì˜ ë¶„ì‚°ì˜ ë³´ì¡´ìˆ˜ì¤€ì´ ë†’ì•„ì§<br>
<br>ğŸ’  PCA ì ˆì°¨
<br>
í•™ìŠµ ë°ì´í„°ì…‹ì—ì„œ ë¶„ì‚°ì´ ìµœëŒ€ì¸ ì¶•(axis)ì„ ì°¾ìŒ<br>
ì²«ë²ˆì§¸ ì¶•ê³¼ ì§êµ(orthogonal)í•˜ë©´ì„œ ë¶„ì‚°ì´ ìµœëŒ€ì¸ ë‘ ë²ˆì§¸ ì¶•ì„ ì°¾ìŒ<br>
ì²« ë²ˆì§¸ ì¶•ê³¼ ë‘ ë²ˆì§¸ ì¶•ì— ì§êµí•˜ê³  ë¶„ì‚°ì„ ìµœëŒ€í•œ ë³´ì¡´í•˜ëŠ” ì„¸ ë²ˆì§¸ ì¶•ì„ ì°¾ìŒ<br>
1~3ê³¼ ê°™ì€ ë°©ë²•ìœ¼ë¡œ ë°ì´í„°ì…‹ì˜ ì°¨ì›(íŠ¹ì„± ìˆ˜)ë§Œí¼ì˜ ì¶•ì„ ì°¾ìŒ<br>
<br>
## 2-3 ì„ í˜•íŒë³„ë¶„ì„
ğŸ’  ì„ í˜•íŒë³„ë¶„ì„(Linear Discriminant Analysis, LDA)<br>
ì…ë ¥ ë°ì´í„° ì„¸íŠ¸ë¥¼ ì €ì°¨ì› ê³µê°„ìœ¼ë¡œ íˆ¬ì˜(projection)í•´ ì°¨ì›ì„ ì¶•ì†Œí•˜ëŠ” ê¸°ë²•<br>
ë°ì´í„°ì˜ Targetê°’ í´ë˜ìŠ¤ë¼ë¦¬ ìµœëŒ€í•œ ë¶„ë¦¬í•  ìˆ˜ ìˆëŠ” ì¶•ì„ ì°¾ìŒ â†’ ì§€ë„ í•™ìŠµ<br>
PCAëŠ” Targetê°’ì„ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ë¹„ì§€ë„ í•™ìŠµ<br>
<br>
ğŸ’  LDA ì ˆì°¨<br>
íŠ¹ì • ê³µê°„ìƒì—ì„œ í´ë˜ìŠ¤ ë¶„ë¦¬ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ì¶•ì„ ì°¾ê¸° ìœ„í•´ í´ë˜ìŠ¤ ê°„ ë¶„ì‚°(between-class scatter)ê³¼ í´ë˜ìŠ¤ ë‚´ë¶€ ë¶„ì‚°(within-class scatter)ì˜ ë¹„ìœ¨ì„ ìµœëŒ€í™”í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì°¨ì›ì„ ì¶•ì†Œ<br>
SVM ê°™ì€ ë‹¤ë¥¸ ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜ì„ ì ìš©í•˜ê¸° ì „ì— ì°¨ì›ì„ ì¶•ì†Œì‹œí‚¤ëŠ” ë° ì‚¬ìš©<br>
## 2-4. Scikit-Learnìœ¼ë¡œ PCAì™€ LDA ìˆ˜í–‰í•˜ê¸°
```python
from sklearn import datasets
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

# iris ë°ì´í„°ì…‹ì„ ë¡œë“œ
iris = datasets.load_iris()

X = iris.data # iris ë°ì´í„°ì…‹ì˜ í”¼ì³ë“¤
y = iris.target # iris ë°ì´í„°ì…‹ì˜ íƒ€ê²Ÿ
target_names = list(iris.target_names) # iris ë°ì´í„°ì…‹ì˜ íƒ€ê²Ÿ ì´ë¦„

print(f'{X.shape = }, {y.shape = }') # 150ê°œ ë°ì´í„°, 4 features
print(f'{target_names = }')  
```

    X.shape = (150, 4), y.shape = (150,)
    target_names = ['setosa', 'versicolor', 'virginica']
    


```python
# PCAì˜ ê°ì²´ë¥¼ ìƒì„±, ì°¨ì›ì€ 2ì°¨ì›ìœ¼ë¡œ ì„¤ì •(í˜„ì¬ëŠ” 4ì°¨ì›)
pca = PCA(n_components=2)

# PCAë¥¼ ìˆ˜í–‰. PCAëŠ” ë¹„ì§€ë„ í•™ìŠµì´ë¯€ë¡œ yê°’ì„ ë„£ì§€ ì•ŠìŒ
pca_fitted = pca.fit(X)

print(f'{pca_fitted.components_ = }')  # ì£¼ì„±ë¶„ ë²¡í„°
print(f'{pca_fitted.explained_variance_ratio_ = }') # ì£¼ì„±ë¶„ ë²¡í„°ì˜ ì„¤ëª…í•  ìˆ˜ ìˆëŠ” ë¶„ì‚° ë¹„ìœ¨

X_pca = pca_fitted.transform(X) # ì£¼ì„±ë¶„ ë²¡í„°ë¡œ ë°ì´í„°ë¥¼ ë³€í™˜
print(f'{X_pca.shape = }')  # 4ì°¨ì› ë°ì´í„°ê°€ 2ì°¨ì› ë°ì´í„°ë¡œ ë³€í™˜ë¨
```

    pca_fitted.components_ = array([[ 0.36138659, -0.08452251,  0.85667061,  0.3582892 ],
           [ 0.65658877,  0.73016143, -0.17337266, -0.07548102]])
    pca_fitted.explained_variance_ratio_ = array([0.92461872, 0.05306648])
    X_pca.shape = (150, 2)
    


```python
# LDAì˜ ê°ì²´ë¥¼ ìƒì„±. ì°¨ì›ì€ 2ì°¨ì›ìœ¼ë¡œ ì„¤ì •(í˜„ì¬ëŠ” 4ì°¨ì›)
lda = LinearDiscriminantAnalysis(n_components=2)

# LDAë¥¼ ìˆ˜í–‰. LDAëŠ” ì§€ë„í•™ìŠµì´ë¯€ë¡œ íƒ€ê²Ÿê°’ì´ í•„ìš”
lda_fitted = lda.fit(X, y)

print(f'{lda_fitted.coef_=}') # LDAì˜ ê³„ìˆ˜
print(f'{lda_fitted.explained_variance_ratio_=}') # LDAì˜ ë¶„ì‚°ì— ëŒ€í•œ ì„¤ëª…ë ¥

X_lda = lda_fitted.transform(X)
print(f'{X_lda.shape = }')  # 4ì°¨ì› ë°ì´í„°ê°€ 2ì°¨ì› ë°ì´í„°ë¡œ ë³€í™˜ë¨
```

    lda_fitted.coef_=array([[  6.31475846,  12.13931718, -16.94642465, -20.77005459],
           [ -1.53119919,  -4.37604348,   4.69566531,   3.06258539],
           [ -4.78355927,  -7.7632737 ,  12.25075935,  17.7074692 ]])
    lda_fitted.explained_variance_ratio_=array([0.9912126, 0.0087874])
    X_lda.shape = (150, 2)
    


```python
# ì‹œê°í™” í•˜ê¸°
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Seabornì„ ì´ìš©í•˜ê¸° ìœ„í•´ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
df_pca = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])
df_lda = pd.DataFrame(X_lda, columns=['LD1', 'LD2'])
y = pd.Series(y).replace({0:'setosa', 1:'versicolor', 2:'virginica'})

# subplotìœ¼ë¡œ ì‹œê°í™”
fig, ax = plt.subplots(1, 2, figsize=(10, 4))

sns.scatterplot(df_pca, x='PC1', y='PC2', hue=y, style=y, ax=ax[0], palette='Set1')
ax[0].set_title('PCA of IRIS dataset')

sns.scatterplot(df_lda, x='LD1', y='LD2', hue=y, style=y, ax=ax[1], palette='Set1')
ax[1].set_title('LDA of IRIS dataset')

plt.show()
```


    
![png](https://drive.google.com/uc?id=1pM6jbyCcKqXHYEXOCwTT3rzr-6lmAh6C)
    


# 3. í”¼ì³ ì„ íƒ ê¸°ë²•
ì¢…ì†ë³€ìˆ˜ í™œìš©ì—¬ë¶€ì— ë”°ë¼<br>
-Supervised: ì¢…ì†ë³€ìˆ˜ë¥¼ í™œìš©í•˜ì—¬ ì„ íƒ<br>
-Unsupervised: ë…ë¦½ë³€ìˆ˜ë“¤ë§Œì„ ì´ìš©í•´ì„œ ì„ íƒ(ì¢…ì†ë³€ìˆ˜ì¸ yê°€ ì—†ìŒ)<br>
<br>
ì„ íƒ ë©”ì»¤ë‹ˆì¦˜ì— ë”°ë¼<br>
-Filter: í†µê³„ì ì¸ ë°©ë²•ìœ¼ë¡œ ì„ íƒ<br>
-Wrapper: ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ì„ íƒ<br>
-Embedded: ëª¨ë¸ í›ˆë ¨ ê³¼ì •ì—ì„œ ìë™ìœ¼ë¡œ ì„ íƒ<br>
-Hybrid: Filter + Wrapper<br>
<br>
## 3-1. í•„í„°ê¸°ë²•(Filter Method)
ëª¨ë“  í”¼ì³ ë°°ì • -> ìµœì ì˜ subset ì„ íƒ -> í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ -> ì„±ëŠ¥í‰ê°€<br>
<br>
-ë°ì´í„°ì˜ í†µê³„ì  ì¸¡ì • ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ ë³€ìˆ˜ë“¤ì˜ ìƒê´€ê´€ê³„ë¥¼ ì•Œì•„ëƒ„<br>
-ê³„ì‚°ì†ë„ê°€ ë¹ ë¥´ê³  ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„ë¥¼ ì•Œì•„ë‚´ëŠ” ë° ì í•©í•˜ì—¬ ë˜í¼ ê¸°ë²•(Wrapper Method)ì„ ì‚¬ìš©í•˜ê¸° ì „ì— ì „ì²˜ë¦¬í•˜ëŠ” ë° ì‚¬ìš©<br>
-íŠ¹ì • ëª¨ë¸ë§ ê¸°ë²•ì— ì˜ì¡´í•˜ì§€ ì•Šê³  ë°ì´í„°ì˜ í†µê³„ì  íŠ¹ì„±ë¶€í„° ë³€ìˆ˜ë¥¼ íƒí•˜ëŠ” ê¸°ë²•<br>
<br>
ğŸ’  í•„í„°ê¸°ë²•ì˜ ì¢…ë¥˜<br>
<br>
-ë¶„ì‚° ê¸°ë°˜ ì„ íƒ(Variance-based Selection): ë¶„ì‚°ì´ ë‚®ì€ ë³€ìˆ˜ë¥¼ ì œê±°í•˜ëŠ” ë°©ë²•<br>
-ì •ë³´ ì†Œë“(Information Gain): ê°€ì¥ ì •ë³´ ì†Œë“ì´ ë†’ì€ ì†ì„±ì„ ì„ íƒí•˜ì—¬ ë°ì´í„°ë¥¼ ë” ì˜ êµ¬ë¶„í•˜ê²Œ ë˜ëŠ” ê²ƒ<br>
-ì¹´ì´ì œê³± ê²€ì •(Chi-Square Test): ì¹´ì´ì œê³± ë¶„í¬$Ï‡_2$ì— ê¸°ì´ˆí•œ í†µê³„ì  ë°©ë²•ìœ¼ë¡œ ê´€ì°°ëœ ë¹ˆë„ê°€ ê¸°ëŒ€ë˜ëŠ” ë¹ˆë„ì™€ ì˜ë¯¸ìˆê²Œ ë‹¤ë¥¸ì§€ ì—¬ë¶€ë¥¼ ê²€ì¦í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë˜ëŠ” ê²€ì¦ ë°©ë²•<br>
-í”¼ì…” ìŠ¤ì½”ì–´(Fisher Score): ìµœëŒ€ ê°€ëŠ¥ì„± ë°©ì •ì‹ì„ í’€ê¸° ìœ„í•´ í†µê³„ì— ì‚¬ìš©ë˜ëŠ” ë‰´í„´(Newton)ì˜ ë°©ë²•<br>
-ìƒê´€ê³„ìˆ˜(Correlation Coefficient): ë‘ ë³€ìˆ˜ ì‚¬ì´ì˜ í†µê³„ì  ê´€ê³„ë¥¼ í‘œí˜„í•˜ê¸° ìœ„í•´ íŠ¹ì •í•œ ìƒê´€ê´€ê³„ì˜ ì •ë„ë¥¼ ìˆ˜ì¹˜ì ìœ¼ë¡œ ë‚˜íƒ€ë‚¸ ê³„ìˆ˜<br>

```python
from sklearn import datasets
from sklearn.feature_selection import VarianceThreshold

# iris ë°ì´í„°ì…‹ì„ ë¡œë“œ
iris = datasets.load_iris()

X = iris.data # iris ë°ì´í„°ì…‹ì˜ í”¼ì³ë“¤
y = iris.target # iris ë°ì´í„°ì…‹ì˜ íƒ€ê²Ÿ
X_names = iris.feature_names # iris ë°ì´í„°ì…‹ì˜ í”¼ì³ ì´ë¦„
y_names = iris.target_names # iris ë°ì´í„°ì…‹ì˜ íƒ€ê²Ÿ ì´ë¦„

# ë¶„ì‚°ì´ 0.2 ì´ìƒì¸ í”¼ì³ë“¤ë§Œ ì„ íƒí•˜ë„ë¡ í•™ìŠµ
sel = VarianceThreshold(threshold=0.2).fit(X)
print(f'{sel.variances_ = }') # ê° í”¼ì³ì˜ ë¶„ì‚° í™•ì¸

# ë¶„ì‚°ì´ 0.2 ì´ìƒì¸ í”¼ì³ë“¤ë§Œ ì„ íƒ ì ìš©
X_selected = sel.transform(X) # ë¶„ì‚°ì´ 0.2 ì´ìƒì¸ í”¼ì³ë“¤ë§Œ ì„ íƒ
X_selected_names = [X_names[i] for i in sel.get_support(indices=True)] # ì„ íƒëœ í”¼ì³ë“¤ì˜ ì´ë¦„

print(f'{X_selected_names = }')
print(f'{X_selected[:5] = }')
```

    sel.variances_ = array([0.68112222, 0.18871289, 3.09550267, 0.57713289])
    X_selected_names = ['sepal length (cm)', 'petal length (cm)', 'petal width (cm)']
    X_selected[:5] = array([[5.1, 1.4, 0.2],
           [4.9, 1.4, 0.2],
           [4.7, 1.3, 0.2],
           [4.6, 1.5, 0.2],
           [5. , 1.4, 0.2]])
    
ğŸ’  Scikit-Learn ì œê³µ í”¼ì³ ì„ íƒ ë©”ì„œë“œ<br>
<br>
-SelectKBest(): ê³ ì •ëœ kê°œì˜ í”¼ì³ ì„ íƒê¸°<br>
-SelectPercentile(): ë¶„ìœ„ìˆ˜ ê¸°ë°˜ ì„ íƒê¸°<br>
-SelectFpr(): False positive rate ê¸°ë°˜ ì„ íƒê¸°<br>
-SelectFdr(): ì¶”ì •ëœ False discovery rate ê¸°ë°˜ ì„ íƒê¸°<br>
-SelectFwe(): familiy-wise error rate ê¸°ë°˜ ì„ íƒê¸°<br>
-GenericUnivariateSelect(): ë‹¨ë³€ëŸ‰ í”¼ì³ ì„ íƒê¸°<br>
ğŸ’  Scikit-Learn ì œê³µ í”¼ì³ ì„ íƒ ê¸°ì¤€<br>
<br>
f_classif: ANOVA F-value ë¶„ë¥˜<br>
mutual_info_classif: ìƒí˜¸ì •ë³´ëŸ‰(mutual information) ë¶„ë¥˜<br>
chi2: ì¹´ì´ì œê³± ë¶„ë¥˜<br>
f_regression: F-value íšŒê·€<br>
mutual_info_regression: ìƒí˜¸ì •ë³´ëŸ‰(mutual information) íšŒê·€<br>

```python
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif, f_regression, chi2

# kê°œì˜ ë² ìŠ¤íŠ¸ í”¼ì³ë¥¼ ì„ íƒ
sel_fc = SelectKBest(f_classif, k=2).fit(X, y)
print('f_classif: ')
print(f'{sel_fc.scores_ = }')
print(f'{sel_fc.pvalues_ = }')
print(f'{sel_fc.get_support() = }')
print('Selected features: ', [X_names[i] for i in sel_fc.get_support(indices=True)]) # ì„ íƒëœ í”¼ì³ë“¤ì˜ ì´ë¦„

sel_fr = SelectKBest(f_regression, k=2).fit(X, y)
print('\nf_regression: ')
print(f'{sel_fr.scores_ = }')
print(f'{sel_fr.pvalues_ = }')
print(f'{sel_fr.get_support() = }')
print('Selected features: ', [X_names[i] for i in sel_fr.get_support(indices=True)]) # ì„ íƒëœ í”¼ì³ë“¤ì˜ ì´ë¦„

sel_chi2 = SelectKBest(chi2, k=2).fit(X, y)
print('\nchi2: ')
print(f'{sel_chi2.scores_ = }')
print(f'{sel_chi2.pvalues_ = }')
print(f'{sel_chi2.get_support() = }')
print('Selected features: ', [X_names[i] for i in sel_chi2.get_support(indices=True)]) # ì„ íƒëœ í”¼ì³ë“¤ì˜ ì´ë¦„
```

    f_classif: 
    sel_fc.scores_ = array([ 119.26450218,   49.16004009, 1180.16118225,  960.0071468 ])
    sel_fc.pvalues_ = array([1.66966919e-31, 4.49201713e-17, 2.85677661e-91, 4.16944584e-85])
    sel_fc.get_support() = array([False, False,  True,  True])
    Selected features:  ['petal length (cm)', 'petal width (cm)']
    
    f_regression: 
    sel_fr.scores_ = array([ 233.8389959 ,   32.93720748, 1341.93578461, 1592.82421036])
    sel_fr.pvalues_ = array([2.89047835e-32, 5.20156326e-08, 4.20187315e-76, 4.15531102e-81])
    sel_fr.get_support() = array([False, False,  True,  True])
    Selected features:  ['petal length (cm)', 'petal width (cm)']
    
    chi2: 
    sel_chi2.scores_ = array([ 10.81782088,   3.7107283 , 116.31261309,  67.0483602 ])
    sel_chi2.pvalues_ = array([4.47651499e-03, 1.56395980e-01, 5.53397228e-26, 2.75824965e-15])
    sel_chi2.get_support() = array([False, False,  True,  True])
    Selected features:  ['petal length (cm)', 'petal width (cm)']
    

## 3-2. ë˜í¼ ê¸°ë²•(Wrapper Method)
ì˜ˆì¸¡ ì •í™•ë„ ì¸¡ë©´ì—ì„œ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” í•˜ìœ„ ì§‘í•©ì„ ì„ íƒí•˜ëŠ” ê¸°ë²•<br>
ê²€ìƒ‰ ê°€ëŠ¥í•œ ë°©ë²•ìœ¼ë¡œ í•˜ìœ„ ì§‘í•©ì„ ë°˜ë³µí•´ì„œ ì„ íƒí•˜ì—¬ í…ŒìŠ¤íŠ¸í•˜ëŠ” ê²ƒì´ë¯€ë¡œ íƒìš• ì•Œê³ ë¦¬ì¦˜(Greedy Algorithm)ì— ì†í•¨<br>
ë°˜ë³µí•˜ì—¬ ì„ íƒí•˜ëŠ” ë°©ë²•ìœ¼ë¡œ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ê³  ë¶€ë¶„ì§‘í•©ì˜ ìˆ˜ê°€ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ëŠ˜ì–´ ê³¼ì í•©ì˜ ìœ„í—˜ì´ ë°œìƒí•  ìˆ˜ ìˆìŒ<br>
ì¼ë°˜ì ìœ¼ë¡œ ë˜í¼ ë°©ë²•ì€ í•„í„° ë°©ë²•ë³´ë‹¤ ì˜ˆì¸¡ ì •í™•ë„ê°€ ë†’ìŒ<br>
<br>
ğŸ’  ë³€ìˆ˜ ì„ íƒì„ ìœ„í•œ ì•Œê³ ë¦¬ì¦˜<br>
-ì „ì§„ ì„ íƒë²•(Forward Selection): ëª¨í˜•ì„ ê°€ì¥ ë§ì´ í–¥ìƒì‹œí‚¤ëŠ” ë³€ìˆ˜ë¥¼ í•˜ë‚˜ì”© ì ì§„ì ìœ¼ë¡œ ì¶”ê°€í•˜ëŠ” ë°©ë²•<br>
-í›„ì§„ ì œê±°ë²•(Backward Elimination): ëª¨ë‘ í¬í•¨ëœ ìƒíƒœì—ì„œ ì‹œì‘í•˜ì—¬ ê°€ì¥ ì ì€ ì˜í–¥ì„ ì£¼ëŠ” ë³€ìˆ˜ë¶€í„° í•˜ë‚˜ì”© ì œê±°<br>
-ë‹¨ê³„ì  ë°©ë²•(Stepwise Method): ì „ì§„ì„ íƒê³¼ í›„í–¥ì œê±°ì˜ ê²°í•©/ ê° ë‹¨ê³„ì—ì„œ ìµœìƒì˜ ì†ì„±ì„ ì„ íƒí•˜ê³  ë‚˜ë¨¸ì§€ ì†ì„± ì¤‘ ìµœì•…ì˜ ì†ì„±ì„ ì œê±°í•˜ëŠ” ê³¼ì •ì„ ì‹¤í–‰<br>
-ì˜ì‚¬ê²°ì •íŠ¸ë¦¬<br>
<br>
ğŸ’  ë˜í¼ê¸°ë²•ì˜ ì¢…ë¥˜<br>
-RFE(Recursive Feature Elimination): SVM(Support Vector Machine)ì„ ì‚¬ìš©í•˜ì—¬ ì¬ê·€ì ìœ¼ë¡œ ì œê±°í•˜ëŠ” ë°©ë²•/ ì „ì§„ ì„ íƒ, í›„ì§„ ì œê±°, ë‹¨ê³„ì  ë°©ë²• ì‚¬ìš©<br>
-SFS(Sequential Feature Selection): ê·¸ë¦¬ë”” ì•Œê³ ë¦¬ì¦˜(Greedy Algorithm)ìœ¼ë¡œ ë¹ˆ ë¶€ë¶„ ì§‘í•©ì—ì„œ íŠ¹ì„± ë³€ìˆ˜ë¥¼ í•˜ë‚˜ì”© ì¶”ê°€í•˜ëŠ” ë°©ë²• /ì „ì§„ ì„ íƒ, í›„ì§„ ì œê±° ì‚¬ìš©<br>

```python
# RFE(Recursive Feature Elimination) ì ìš©
from sklearn.datasets import load_iris
from sklearn.feature_selection import RFE, RFECV, SelectFromModel, SequentialFeatureSelector
from sklearn.svm import SVC, SVR

# iris ë°ì´í„°ì…‹ ë¡œë“œ
X, y = load_iris(return_X_y=True)

# ë¶„ë¥˜ê¸° SVC ê°ì²´ ìƒì„±, ì„ í˜•ë¶„ë¥˜, 3ê°œì˜ í´ë˜ìŠ¤ 
svc = SVR(kernel="linear", C=3)

# RFE ê°ì²´ ìƒì„±, 2ê°œì˜ í”¼ì³ ì„ íƒ, 1ê°œì”© ì œê±° 
rfe = RFE(estimator=svc, n_features_to_select=2, step=1)
# RFE+CV(Cross Validation), 5ê°œì˜ í´ë“œ, 1ê°œì”© ì œê±°
rfe_cv = RFECV(estimator=svc, step=1, cv=5) 

# ë°ì´í„°ì…‹ì— RFE ì ìš©
rfe.fit(X, y)
print('RFE Rank: ', rfe.ranking_)

# rankê°€ 1ì¸ í”¼ì³ë“¤ë§Œ ì„ íƒ
X_selected = rfe.transform(X) 
X_selected_names = [X_names[i] for i in rfe.get_support(indices=True)] # ì„ íƒëœ í”¼ì³ë“¤ì˜ ì´ë¦„

print(f'{X_selected_names = }')
print(f'{X_selected[:5] = }')

# ë°ì´í„°ì…‹ì— RFECV ì ìš©
rfe_cv.fit(X, y)
print('RFECV Rank: ', rfe_cv.ranking_)

# rankê°€ 1ì¸ í”¼ì³ë“¤ë§Œ ì„ íƒ
X_selected = rfe_cv.transform(X) 
X_selected_names = [X_names[i] for i in rfe_cv.get_support(indices=True)] # ì„ íƒëœ í”¼ì³ë“¤ì˜ ì´ë¦„

print(f'{X_selected_names = }')
print(f'{X_selected[:5] = }')
```

    RFE Rank:  [2 3 1 1]
    X_selected_names = ['petal length (cm)', 'petal width (cm)']
    X_selected[:5] = array([[1.4, 0.2],
           [1.4, 0.2],
           [1.3, 0.2],
           [1.5, 0.2],
           [1.4, 0.2]])
    RFECV Rank:  [1 2 1 1]
    X_selected_names = ['sepal length (cm)', 'petal length (cm)', 'petal width (cm)']
    X_selected[:5] = array([[5.1, 1.4, 0.2],
           [4.9, 1.4, 0.2],
           [4.7, 1.3, 0.2],
           [4.6, 1.5, 0.2],
           [5. , 1.4, 0.2]])
    


```python
# SFS(Sequential Feature Selector) : ìˆœì°¨ì ìœ¼ë¡œ íŠ¹ì„±ì„ ì„ íƒí•˜ëŠ” ë°©ë²•

from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import load_iris

# ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³ , ë¶„ë¥˜ê¸°ë¥¼ ì´ˆê¸°í™”í•œ í›„ SFSë¥¼ ì ìš©
X, y = load_iris(return_X_y=True)
knn = KNeighborsClassifier(n_neighbors=3)
sfs = SequentialFeatureSelector(knn, n_features_to_select=2, direction='backward')

# SFSë¥¼ í•™ìŠµí•˜ê³ , ì„ íƒëœ íŠ¹ì„±ì„ ì¶œë ¥
sfs.fit(X, y)
print('SFS selected: ', sfs.get_support())

# ì„ íƒëœ í”¼ì³ë“¤ë§Œ ì„ íƒ
X_selected = sfs.transform(X) 
X_selected_names = [X_names[i] for i in sfs.get_support(indices=True)] # ì„ íƒëœ í”¼ì³ë“¤ì˜ ì´ë¦„

print(f'{X_selected_names = }')
print(f'{X_selected[:5] = }')
```

    SFS selected:  [False False  True  True]
    X_selected_names = ['petal length (cm)', 'petal width (cm)']
    X_selected[:5] = array([[1.4, 0.2],
           [1.4, 0.2],
           [1.3, 0.2],
           [1.5, 0.2],
           [1.4, 0.2]])
    

## 3-3. ì„ë² ë””ë“œ ê¸°ë²•(Embedded Method)
ì„ë² ë””ë“œ ê¸°ë²•ì€ ëª¨ë¸ì˜ ì •í™•ë„ì— ê¸°ì—¬í•˜ëŠ” ë³€ìˆ˜ë¥¼ í•™ìŠµí•¨<Br>
SelectFromModel<br>
->ì˜ì‚¬ê²°ì •ë‚˜ë¬´ ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ì—ì„œ ë³€ìˆ˜ë¥¼ ì„ íƒí•˜ëŠ” ê¸°ë²•<br>
```python
from sklearn.feature_selection import SelectFromModel
from sklearn import tree
from sklearn.datasets import load_iris

# ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³ , ë¶„ë¥˜ê¸°ë¥¼ ì´ˆê¸°í™”í•œ í›„ SFSë¥¼ ì ìš©
X, y = load_iris(return_X_y=True)
clf = tree.DecisionTreeClassifier()
sfm = SelectFromModel(estimator=clf)

# ëª¨í˜• êµ¬ì¡° í™•ì¸ ë° ì¶œë ¥ì„ pandasë¡œ ì„¤ì •
sfm.set_output(transform='pandas')
```




<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: "â–¸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "â–¾";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>SelectFromModel(estimator=DecisionTreeClassifier())</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" ><label for="sk-estimator-id-1" class="sk-toggleable__label sk-toggleable__label-arrow">SelectFromModel</label><div class="sk-toggleable__content"><pre>SelectFromModel(estimator=DecisionTreeClassifier())</pre></div></div></div><div class="sk-parallel"><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-2" type="checkbox" ><label for="sk-estimator-id-2" class="sk-toggleable__label sk-toggleable__label-arrow">estimator: DecisionTreeClassifier</label><div class="sk-toggleable__content"><pre>DecisionTreeClassifier()</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-3" type="checkbox" ><label for="sk-estimator-id-3" class="sk-toggleable__label sk-toggleable__label-arrow">DecisionTreeClassifier</label><div class="sk-toggleable__content"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div></div></div></div></div></div>




```python
# ëª¨í˜• í•™ìŠµ
sfm.fit(X, y)
print('SFM threshold: ', sfm.threshold_)

# ì„ íƒëœ í”¼ì³ë“¤ë§Œ ì„ íƒ
X_selected = sfm.transform(X) 
X_selected.columns = [X_names[i] for i in sfm.get_support(indices=True)] # ì„ íƒëœ í”¼ì³ë“¤ì˜ ì´ë¦„

X_selected.head()
```

    SFM threshold:  0.25
    




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>petal width (cm)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.2</td>
    </tr>
  </tbody>
</table>
</div>


